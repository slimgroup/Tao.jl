<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distribution · ParametricOperators.jl</title><meta name="title" content="Distribution · ParametricOperators.jl"/><meta property="og:title" content="Distribution · ParametricOperators.jl"/><meta property="twitter:title" content="Distribution · ParametricOperators.jl"/><meta name="description" content="Documentation for ParametricOperators.jl."/><meta property="og:description" content="Documentation for ParametricOperators.jl."/><meta property="twitter:description" content="Documentation for ParametricOperators.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ParametricOperators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ParametricOperators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li class="is-active"><a class="tocitem" href>Distribution</a><ul class="internal"><li><a class="tocitem" href="#Kronecker-Distribution"><span>Kronecker Distribution</span></a></li><li><a class="tocitem" href="#Sharing-Weights"><span>Sharing Weights</span></a></li><li><a class="tocitem" href="#Reduction-Operation"><span>Reduction Operation</span></a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/3D_FFT/">3D FFT</a></li><li><a class="tocitem" href="../examples/3D_DFFT/">Distributed 3D FFT</a></li><li><a class="tocitem" href="../examples/3D_Conv/">3D Conv</a></li><li><a class="tocitem" href="../examples/3D_DConv/">Distributed 3D Conv</a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../future/">Future Work</a></li><li><a class="tocitem" href="../citation/">Citation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Distribution</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distribution</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/slimgroup/ParametricOperators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/slimgroup/ParametricOperators.jl/blob/main/docs/src/distribution.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Distribution-as-Linear-Algebra"><a class="docs-heading-anchor" href="#Distribution-as-Linear-Algebra">Distribution as Linear Algebra</a><a id="Distribution-as-Linear-Algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Distribution-as-Linear-Algebra" title="Permalink"></a></h1><p>We adapt an approach of looking at distribution of tensor computation as Linear Algebra operations. </p><p>This allows <code>ParametricOperators.jl</code> to offer several high level API in order to perform controlled parallelism as part of your tensor program in the context of machine learning.</p><h2 id="Kronecker-Distribution"><a class="docs-heading-anchor" href="#Kronecker-Distribution">Kronecker Distribution</a><a id="Kronecker-Distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Kronecker-Distribution" title="Permalink"></a></h2><h3 id="Distributed-Fourier-Transform"><a class="docs-heading-anchor" href="#Distributed-Fourier-Transform">Distributed Fourier Transform</a><a id="Distributed-Fourier-Transform-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Fourier-Transform" title="Permalink"></a></h3><p>Let&#39;s consider the example of Fourier Transform as seen in the <a href="../quickstart/#Fourier-Transform-Example">Fourier Transform Example</a> </p><pre><code class="language-julia hljs"># Define type and the size of our global tensor
T = Float32
gx, gy, gz = 10, 20, 30

Fx = ParDFT(T, gx)
Fy = ParDFT(Complex{T}, gy)
Fz = ParDFT(Complex{T}, gz)

F = Fz ⊗ Fy ⊗ Fx</code></pre><p>Assume that our data is partitioned across multiple machine according to the following scheme:</p><pre><code class="language-julia hljs">partition = [1, 1, 2]</code></pre><p>Each element of <code>partition</code> denotes the number of processing elements that divide our input tensor along that dimension.</p><p>For eg. given the above partition and global size, our local tensor would be of size:</p><pre><code class="language-julia hljs">x = rand(T, 10, 20, 15)</code></pre><p>OR in other terms:</p><pre><code class="language-julia hljs">localx, localy, localz = [gx, gy, gz] .÷ partition
x = rand(T, localx, localy, localz)</code></pre><p>Now, following the method seen in several recent works (Grady et al., <a href="https://arxiv.org/pdf/2204.01205.pdf">2022</a>) and <a href="https://jipolanco.github.io/PencilFFTs.jl/dev/tutorial/">traditional distributed FFTs</a>, we can distribute the application of our linearly separable transform across multiple processing elements by simply doing:</p><pre><code class="language-julia hljs">F = distribute(F, partition)</code></pre><p>Now, to apply the Fourier Transform to our tensor, one can do:</p><pre><code class="language-julia hljs">F * vec(x)</code></pre><p>Another out-of-box example can be seen at <a href="../examples/3D_DFFT/#Distributed-FFT-of-a-3D-Tensor">Distributed FFT of a 3D Tensor</a></p><h3 id="Distributed-Convolution"><a class="docs-heading-anchor" href="#Distributed-Convolution">Distributed Convolution</a><a id="Distributed-Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Convolution" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Definition of Convolution</header><div class="admonition-body"><p>Convolution here refers to the application of a linear transform along the channel dimension</p></div></div><p>Now, in order to extend this to a convolution layer, lets consider the following partitioned tensor:</p><pre><code class="language-julia hljs">T = Float32

gx, gy, gc = 10, 30, 50
partition = [2, 2, 1]

nx, ny, nc = [gx, gy, gc] .÷ partition
x = rand(T, nx, ny, nc)</code></pre><p>Our tensor is sharded across x and y dimensions by 2 processing element along each dimension. </p><p>We can define the operators of our convolution as:</p><pre><code class="language-julia hljs">Sx = ParIdentity(T, gx)
Sy = ParIdentity(T, gy)
Sc = ParMatrix(T, gc, gc)</code></pre><p>Chain our operators and distribute them:</p><pre><code class="language-julia hljs">S = Sc ⊗ Sy ⊗ Sx
S = distribute(S, partition)</code></pre><p>Parametrize and apply our transform:</p><pre><code class="language-julia hljs">θ = init(S)
S(θ) * vec(x)</code></pre><p>Take the gradient of the parameters w.r.t to some objective by simply doing:</p><pre><code class="language-julia hljs">θ′ = gradient(θ -&gt; sum(S(θ) * vec(x)), θ)</code></pre><p>Another out-of-box example can be seen at <a href="../examples/3D_DConv/#Distributed-Parametrized-Convolution-of-a-3D-Tensor">Distributed Parametrized Convolution of a 3D Tensor</a></p><h2 id="Sharing-Weights"><a class="docs-heading-anchor" href="#Sharing-Weights">Sharing Weights</a><a id="Sharing-Weights-1"></a><a class="docs-heading-anchor-permalink" href="#Sharing-Weights" title="Permalink"></a></h2><p>Sharing weights can be thought of as a broadcasting operation.</p><p>In order to share weights of an operator across multiple processing elements, we can do:</p><pre><code class="language-julia hljs">A = ParMatrix(T, 20, 20)
A = distribute(A)</code></pre><p>Assume the following partition and tensor shape:</p><pre><code class="language-julia hljs">gc, gx = 20, 100
partition = [1, 4]

nc, nx = [gc, gx] .÷ partition
x = rand(T, nc, nx)</code></pre><p>Initialize and apply the matrix operator on the sharded tensor:</p><pre><code class="language-julia hljs">θ = init(A)
A(θ) * x</code></pre><p>Compute the gradient by doing:</p><pre><code class="language-julia hljs">θ′ = gradient(θ -&gt; sum(A(θ) * x), θ)</code></pre><h2 id="Reduction-Operation"><a class="docs-heading-anchor" href="#Reduction-Operation">Reduction Operation</a><a id="Reduction-Operation-1"></a><a class="docs-heading-anchor-permalink" href="#Reduction-Operation" title="Permalink"></a></h2><p>In order to perform a reduction operation, more commonly known as an <code>ALL_REDUCE</code> operation, we can define:</p><pre><code class="language-julia hljs">R = ParReduce(T)</code></pre><p>Given any local vector or matrix, we can do:</p><pre><code class="language-julia hljs">x = rand(T, 100)
R * x</code></pre><p>To compute the gradient of the input w.r.t some objective:</p><pre><code class="language-julia hljs">x′ = gradient(x -&gt; sum(R * x), x)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quickstart/">« Quick Start</a><a class="docs-footer-nextpage" href="../examples/3D_FFT/">3D FFT »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Monday 15 April 2024 15:01">Monday 15 April 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
